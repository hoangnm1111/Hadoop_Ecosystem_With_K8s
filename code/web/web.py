import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import folium
from streamlit_folium import st_folium
import geopandas as gpd
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, hour, dayofweek, dayofmonth, month, year, to_timestamp
from pyspark.ml import PipelineModel
from pyhive import hive
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
import pyspark.sql.functions as F
import os
import plotly.express as px
#from airflow.hooks.base import BaseHook
import json
import pickle
import uuid

# Th√¥ng tin ƒëƒÉng nh·∫≠p c·ªßa admin
USERNAME = "admin"
PASSWORD= "123456"

# H√†m ki·ªÉm tra ƒëƒÉng nh·∫≠p
def check_login(username, password):
    return username == USERNAME and password == PASSWORD

def main_page():
    st.set_page_config(layout="wide")

    padding_top = 0

    st.markdown(f"""
        <style>
            .reportview-container .main .block-container{{
                padding-top: {padding_top}rem;
            }}
        </style>""",
                unsafe_allow_html=True,
                )

    st.markdown("""
            <style>
                   .block-container {
                        padding-top: 1rem;
                        padding-bottom: 0rem;
                        padding-left: 5rem;
                        padding-right: 5rem;
                    }
            </style>
            """, unsafe_allow_html=True)

    st.markdown("""
    <style>
        /* Custom metric cards */
        [data-testid="stMetric"] {
            background-color: #f0f2f6;
            border-radius: 10px;
            padding: 15px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }

        /* Metric value */
        [data-testid="stMetricValue"] {
            font-size: 24px;
            color: #2e86c1;
        }

        /* Metric label */
        [data-testid="stMetricLabel"] {
            font-size: 14px;
            color: #5d6d7e;
        }
    </style>
    """, unsafe_allow_html=True)

    st.markdown("""
    <style>
        /* Style cho expander */
        .st-emotion-cache-1hynsf2 {
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 10px;
            margin-bottom: 10px;
        }
        /* Style cho n√∫t download */
        .stDownloadButton button {
            background-color: #4CAF50 !important;
            color: white !important;
        }
    </style>
    """, unsafe_allow_html=True)

    st.markdown("""
    <style>
        /* Style cho tabs */
        .stTabs [data-baseweb="tab-list"] {
            gap: 10px;
        }
        .stTabs [data-baseweb="tab"] {
            padding: 8px 16px;
            border-radius: 4px 4px 0 0;
        }
        /* Style cho expander */
        .st-emotion-cache-1hynsf2 {
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 10px;
        }
    </style>
    """, unsafe_allow_html=True)

    # Initialize SparkSession
    spark = SparkSession.builder \
        .appName("TaxiAnalysPrediction") \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "4g") \
        .config("spark.executor.memoryOverhead", "1g") \
        .getOrCreate()

    # File paths
    DATA_PATH = "/home/data/yellow_tripdata_{year}-{month}.csv"
    ZONE_LOOKUP_PATH = "/home/explore/taxi+_zone_lookup.csv"
    GEOJSON_PATH = "/home/explore/NYC_Taxi_Zones.geojson"
    # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n m√¥ h√¨nh
    BOROUGH_MODEL_PATH = "hdfs://hadoop-hdfs-hdfs-nn:9000/models/RF/borough_model"
    PC_MODEL_PATH = "hdfs://hadoop-hdfs-hdfs-nn:9000/models/RF/passenger_count_model"

    # Th√™m h√†m k·∫øt n·ªëi Spark Thrift Server
    @st.cache_resource
    def get_spark_thrift_connection():
        try:
            conn = hive.connect(
                host="hadoop-hdfs-hdfs-nn",  # Thay b·∫±ng host c·ªßa b·∫°n
                port=8989,  # Thay b·∫±ng port ph√π h·ª£p
                # username="",
                auth="NONE"
            )
            return conn
        except Exception as e:
            st.error(f"L·ªói khi k·∫øt n·ªëi ƒë·∫øn Spark Thrift Server: {e}")
            return None

    # H√†m truy v·∫•n d·ªØ li·ªáu t·ª´ Iceberg
    @st.cache_data
    def query_iceberg_data(start_date, end_date):
        try:
            conn = get_spark_thrift_connection()
            if conn:
                cursor = conn.cursor()

                query = f"""
                SELECT 
                    tpep_pickup_datetime,
                    tpep_dropoff_datetime,
                    passenger_count,
                    trip_distance,
                    PULocationID,
                    DOLocationID,
                    fare_amount,
                    tip_amount,
                    total_amount
                FROM iceberg.hoangnn7.all_taxi_data
                WHERE tpep_pickup_datetime BETWEEN '{start_date}' AND '{end_date}'
                """

                cursor.execute(query)
                columns = [desc[0] for desc in cursor.description]
                data = cursor.fetchall()

                df = pd.DataFrame(data, columns=columns)

                # Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu datetime
                df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])
                df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])

                return df
            return None
        except Exception as e:
            st.error(f"L·ªói khi truy v·∫•n d·ªØ li·ªáu t·ª´ Iceberg: {e}")
            return None

    # H√†m ƒë·ªçc d·ªØ li·ªáu t·ª´ file CSV
    @st.cache_data
    def load_data(year, month):
        file_path = DATA_PATH.format(year=year, month=f"{month}")
        try:
            df = pd.read_csv(file_path)
            df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], errors='coerce')
            df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'], errors='coerce')
            return df
        except FileNotFoundError:
            st.error(f"File {file_path} kh√¥ng t·ªìn t·∫°i!")
            return None

    # H√†m ƒë·ªçc file taxi_zone_lookup
    @st.cache_data
    def load_zone_lookup():
        try:
            zones = pd.read_csv(ZONE_LOOKUP_PATH)
            return zones
        except FileNotFoundError:
            st.error("File taxi_zone_lookup.csv kh√¥ng t·ªìn t·∫°i!")
            return None

    # H√†m ƒë·ªçc file GeoJSON
    @st.cache_data
    def load_geojson():
        try:
            geo_data = gpd.read_file(GEOJSON_PATH)
            return geo_data
        except FileNotFoundError:
            st.error(
                "File NYC_Taxi_Zones.geojson kh√¥ng t·ªìn t·∫°i! Vui l√≤ng t·∫£i t·ª´ https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddkc")
            return None

    # H√†m ki·ªÉm tra d·ªØ li·ªáu b·ªã nhi·ªÖu
    def check_noisy_data(df):
        noise_info = {}
        df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], errors='coerce')
        df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'], errors='coerce')

        trip_duration = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60
        invalid_time = df[
            (df['tpep_dropoff_datetime'] < df['tpep_pickup_datetime']) |
            (trip_duration > 1440) |
            (trip_duration < 0)
            ]
        noise_info['invalid_time'] = {
            'count': len(invalid_time),
            'indices': invalid_time.index.tolist()
        }

        invalid_distance = df[
            (df['trip_distance'] < 0) |
            (df['trip_distance'] > 100)
            ]
        noise_info['invalid_distance'] = {
            'count': len(invalid_distance),
            'indices': invalid_distance.index.tolist()
        }

        invalid_passengers = df[
            (df['passenger_count'] <= 0) |
            (df['passenger_count'] > 6)
            ]
        noise_info['invalid_passengers'] = {
            'count': len(invalid_passengers),
            'indices': invalid_passengers.index.tolist()
        }

        invalid_fare = df[
            (df['fare_amount'] < 0) |
            (df['total_amount'] < 0) |
            (df['fare_amount'] > 1000)
            ]
        noise_info['invalid_fare'] = {
            'count': len(invalid_fare),
            'indices': invalid_fare.index.tolist()
        }

        return noise_info

    # ƒê·ªãnh nghƒ©a c√°c ƒë·ªô ƒëo hi·ªáu su·∫•t ƒë∆∞·ª£c nh·∫≠p s·∫µn cho t·ª´ng qu·∫≠n
    HARDCODED_METRICS = {
        'Manhattan': {'mae': 43.345, 'mse': 4492.4921, 'rmse': 22.3154, 'r2': 0.5283},
        'Bronx': {'mae': 2.1145, 'mse': 8.4393, 'rmse': 2.9051, 'r2': 0.6782},
        'Queens': {'mae': 11.8753, 'mse': 224.7678, 'rmse': 14.9923, 'r2': 0.5924},
        'Brooklyn': {'mae': 3.3827, 'mse': 21.3098, 'rmse': 4.6131, 'r2': 0.5934},
        'StatenIsland': {'mae': 0.2051, 'mse': 0.1741, 'rmse': 0.4173, 'r2': 0.7912},
        'EWR': {'mae': 0.6876, 'mse': 0.7322, 'rmse': 0.8557, 'r2': 0.7535}
    }
    # H√†m l·∫•y danh s√°ch tu·∫ßn trong th√°ng
    def get_weeks_in_month(df, year, month):
        df_month = df[
            (df['tpep_pickup_datetime'].dt.year == year) &
            (df['tpep_pickup_datetime'].dt.month == month)
            ]
        if df_month.empty:
            return []
        weeks = df_month['tpep_pickup_datetime'].dt.isocalendar().week.unique()
        return sorted([int(w) for w in weeks])

    # H√†m t√≠nh ng√†y b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c c·ªßa tu·∫ßn
    def get_week_date_range(year, week):
        first_day = datetime(year, 1, 1)
        days_to_monday = (7 - first_day.weekday()) % 7 if first_day.weekday() != 0 else 0
        start_date = first_day + timedelta(days=days_to_monday + (int(week) - 1) * 7)
        end_date = start_date + timedelta(days=6)
        return start_date, end_date

    # H√†m t√≠nh ng√†y cu·ªëi tu·∫ßn
    def get_weekend_dates(year, week):
        start_date, _ = get_week_date_range(year, int(week))
        saturday = start_date + timedelta(days=5)
        sunday = start_date + timedelta(days=6)
        return saturday, sunday

    # H√†m l·ªçc d·ªØ li·ªáu theo tu·∫ßn
    def filter_by_week(df, year, month, week):
        return df[
            (df['tpep_pickup_datetime'].dt.year == year) &
            (df['tpep_pickup_datetime'].dt.month == month) &
            (df['tpep_pickup_datetime'].dt.isocalendar().week == week)
            ]

    # H√†m l·ªçc d·ªØ li·ªáu cu·ªëi tu·∫ßn
    def filter_by_weekend(df, year, month, week):
        week_data = filter_by_week(df, year, month, week)
        return week_data[week_data['tpep_pickup_datetime'].dt.dayofweek.isin([5, 6])]

    # H√†m t·∫£i m√¥ h√¨nh t·ª´ HDFS
    @st.cache_resource
    def load_models():
        try:
            model_borough = PipelineModel.load(BOROUGH_MODEL_PATH)  # Thay ƒë·ªïi t√™n bi·∫øn
            model_pc = PipelineModel.load(PC_MODEL_PATH)

            pu_metrics = {
                'accuracy': 0.85,
                'precision': 0.84,
                'recall': 0.83,
                'f1': 0.835
            }

            pc_metrics = {
                'accuracy': 0.78,
                'precision': 0.77,
                'recall': 0.76,
                'f1': 0.765
            }

            return model_borough, model_pc, pu_metrics, pc_metrics
        except Exception as e:
            st.error(f"L·ªói khi t·∫£i m√¥ h√¨nh t·ª´ HDFS: {e}")
            return None, None, None, None

    @st.cache_data
    # H√†m t·∫£i d·ªØ li·ªáu l·ªãch s·ª≠
    def load_historical_data(borough):
        data_path = f"hdfs://hadoop-hdfs-hdfs-nn:9000/user/hive/warehouse/hoangnn7.db/taxi_trip_summary/data/"
        df = spark.read.parquet(data_path)
        df = df.filter((df.Quan == borough) & (df.Nam == 2020) & (df.Thang.between(1, 6)))
        df = df.orderBy('Thang', 'Ngay', 'Gio')
        pdf = df.toPandas()

        # Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
        pdf['ThoiGian'] = pd.to_datetime(pdf['ThoiGian'])

        # Tr√≠ch xu·∫•t feature t·ª´ ThoiGian
        pdf['Day_of_week'] = pdf['ThoiGian'].dt.dayofweek  # 0: T2, 6: CN
        #pdf['Day_of_month'] = pdf['ThoiGian'].dt.day
        #pdf['Month'] = pdf['ThoiGian'].dt.month

        # Chuy·ªÉn ƒë·ªïi Day_of_week th√†nh 2 categories: 1 (Trong tu·∫ßn), 2 (Cu·ªëi tu·∫ßn)
        pdf['Day_of_week_category'] = pdf['Day_of_week'].apply(lambda x: 1 if x < 5 else 2)

        # Chuy·ªÉn ƒë·ªïi Day_of_month th√†nh 3 categories: 1 (ƒê·∫ßu th√°ng), 2 (Gi·ªØa th√°ng), 3 (Cu·ªëi th√°ng)
        pdf['Day_of_month_category'] = pdf['Ngay'].apply(
            lambda x: 1 if x <= 10 else (2 if x <= 20 else 3)
        )

        # T·∫£i b·ªô chu·∫©n h√≥a
        scaler_path = f"hdfs://hadoop-hdfs-hdfs-nn:9000/models/{borough}/scalers.pkl"
        fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
        local_scaler_path = f"/home/ad/models/{borough}_scalers.pkl"

        # T·∫£i b·ªô chu·∫©n h√≥a t·ª´ HDFS
        if fs.exists(spark._jvm.org.apache.hadoop.fs.Path(scaler_path)):
            fs.copyToLocalFile(
                spark._jvm.org.apache.hadoop.fs.Path(scaler_path),
                spark._jvm.org.apache.hadoop.fs.Path(local_scaler_path)
            )
            with open(local_scaler_path, 'rb') as f:
                scalers = pickle.load(f)
            os.remove(local_scaler_path)
        else:
            # T·∫°o b·ªô chu·∫©n h√≥a m·ªõi n·∫øu kh√¥ng t√¨m th·∫•y
            scalers = {
                'gio': MinMaxScaler().fit(pdf[['Gio']]),
                'sochuyen': MinMaxScaler().fit(pdf[['So_chuyen']]),
                'day_of_week': MinMaxScaler().fit(pdf[['Day_of_week_category']]),
                'day_of_month': MinMaxScaler().fit(pdf[['Day_of_month_category']]),
                'month': MinMaxScaler().fit(pdf[['Thang']])
            }

        # Chu·∫©n h√≥a c√°c ƒë·∫∑c tr∆∞ng
        pdf['Gio_normalized'] = scalers['gio'].transform(pdf[['Gio']])
        pdf['So_chuyen_normalized'] = scalers['sochuyen'].transform(pdf[['So_chuyen']])
        pdf['Day_of_week_normalized'] = scalers['day_of_week'].transform(pdf[['Day_of_week_category']])
        pdf['Day_of_month_normalized'] = scalers['day_of_month'].transform(pdf[['Day_of_month_category']])
        pdf['Month_normalized'] = scalers['month'].transform(pdf[['Thang']])

        return pdf, scalers

    # H√†m t·∫£i m√¥ h√¨nh t·ª´ HDFS
    def load_model_from_hdfs(borough):
        hdfs_model_path = f"hdfs://hadoop-hdfs-hdfs-nn:9000/models/{borough}/{borough}_lstm_model.keras"
        local_model_path = f"/home/ad/models/{borough}_lstm_model.keras"
        fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())

        if not fs.exists(spark._jvm.org.apache.hadoop.fs.Path(hdfs_model_path)):
            raise FileNotFoundError(f"M√¥ h√¨nh cho {borough} kh√¥ng t√¨m th·∫•y t·∫°i {hdfs_model_path}")

        # T·∫£i m√¥ h√¨nh t·ª´ HDFS
        fs.copyToLocalFile(
            spark._jvm.org.apache.hadoop.fs.Path(hdfs_model_path),
            spark._jvm.org.apache.hadoop.fs.Path(local_model_path)
        )

        model = load_model(local_model_path)
        os.remove(local_model_path)
        return model

    # H√†m t·∫£i ƒë·ªô ƒëo hi·ªáu su·∫•t t·ª´ HDFS
    def load_metrics(borough):
        metrics_path = f"hdfs://hadoop-hdfs-hdfs-nn:9000/models/{borough}/metrics.json"
        local_metrics_path = f"/home/ad/models/{borough}_metrics.json"
        fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())

        if not fs.exists(spark._jvm.org.apache.hadoop.fs.Path(metrics_path)):
            return {'mae': 0.0, 'mse': 0.0, 'rmse': 0.0, 'r2': 0.0}

        fs.copyToLocalFile(
            spark._jvm.org.apache.hadoop.fs.Path(metrics_path),
            spark._jvm.org.apache.hadoop.fs.Path(local_metrics_path)
        )

        with open(local_metrics_path, 'r') as f:
            metrics = json.load(f)
        os.remove(local_metrics_path)
        return metrics

    # H√†m d·ª± ƒëo√°n s·ªë chuy·∫øn
    def predict_trips(model, pdf, selected_date, selected_hour):
        sequence_length = 24 * 30  # C·∫≠p nh·∫≠t theo m√¥ h√¨nh LSTM m·ªõi
        features = 5

        # L·∫•y b·ªô chu·∫©n h√≥a v√† DataFrame
        scalers = pdf[1]
        pdf = pdf[0]

        # D·ª± ƒëo√°n cho gi·ªù ƒë∆∞·ª£c ch·ªçn v√† 5 gi·ªù ti·∫øp theo
        predictions = []
        hours = list(range(selected_hour, selected_hour + 6))  # 6 gi·ªù (gi·ªù ch·ªçn + 5 gi·ªù ti·∫øp theo)

        for hour in hours:
            # X·ª≠ l√Ω khi gi·ªù v∆∞·ª£t qu√° 23 (sang ng√†y m·ªõi)
            current_date = pd.to_datetime(selected_date) + timedelta(days=hour // 24)
            current_hour = hour % 24

            # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ ng√†y
            day_of_week = current_date.dayofweek
            day_of_month = current_date.day
            month = current_date.month

            # Chuy·ªÉn ƒë·ªïi Day_of_week th√†nh 2 categories: 1 (Trong tu·∫ßn), 2 (Cu·ªëi tu·∫ßn)
            day_of_week_category = 1 if day_of_week < 5 else 2

            # Chuy·ªÉn ƒë·ªïi Day_of_month th√†nh 3 categories: 1 (ƒê·∫ßu th√°ng), 2 (Gi·ªØa th√°ng), 3 (Cu·ªëi th√°ng)
            day_of_month_category = 1 if day_of_month <= 10 else (2 if day_of_month <= 20 else 3)

            # Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o
            gio_normalized = scalers['gio'].transform(np.array([[current_hour]]))[0][0]
            day_of_week_normalized = scalers['day_of_week'].transform(np.array([[day_of_week_category]]))[0][0]
            day_of_month_normalized = scalers['day_of_month'].transform(np.array([[day_of_month_category]]))[0][0]
            month_normalized = scalers['month'].transform(np.array([[month]]))[0][0]

            # L·∫•y d·ªØ li·ªáu g·∫ßn nh·∫•t
            start_time = pd.to_datetime('2020-01-01')
            recent_data = pdf[(pdf['ThoiGian'] >= start_time) & (pdf['ThoiGian'] < current_date)]
            if len(recent_data) < sequence_length:
                raise ValueError(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu t·ª´ th√°ng 1/2020 ƒë·∫øn tr∆∞·ªõc {current_date} ƒë·ªÉ d·ª± ƒëo√°n.")

            recent_data = recent_data.tail(sequence_length)
            recent_data = recent_data[['So_chuyen_normalized', 'Gio_normalized',
                                       'Day_of_week_normalized', 'Day_of_month_normalized',
                                       'Month_normalized']].copy()

            # C·∫≠p nh·∫≠t h√†ng cu·ªëi c√πng v·ªõi ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o
            recent_data.iloc[-1, recent_data.columns.get_loc('Gio_normalized')] = gio_normalized
            recent_data.iloc[-1, recent_data.columns.get_loc('Day_of_week_normalized')] = day_of_week_normalized
            recent_data.iloc[-1, recent_data.columns.get_loc('Day_of_month_normalized')] = day_of_month_normalized
            recent_data.iloc[-1, recent_data.columns.get_loc('Month_normalized')] = month_normalized

            # Chu·∫©n b·ªã chu·ªói
            sequence = recent_data[['So_chuyen_normalized', 'Gio_normalized',
                                    'Day_of_week_normalized', 'Day_of_month_normalized',
                                    'Month_normalized']].values
            sequence = sequence.reshape(1, sequence_length, features)

            # D·ª± ƒëo√°n
            pred_normalized = model.predict(sequence, verbose=0)
            pred = scalers['sochuyen'].inverse_transform(pred_normalized)
            predictions.append(int(pred[0][0]))

        # T·∫°o DataFrame cho bi·ªÉu ƒë·ªì
        hours_str = [f"{h % 24:02d}:00" for h in hours]
        dates = [current_date + timedelta(hours=h - selected_hour) for h in hours]
        date_hour_str = [f"{d.strftime('%Y-%m-%d')} {h}" for d, h in zip(dates, hours_str)]

        return predictions[0], predictions, date_hour_str

    # Class k·∫øt h·ª£p m√¥ h√¨nh
    class CombinedModel:
        def __init__(self, model_borough, model_pc):
            self.model_borough = model_borough
            self.model_pc = model_pc

        def transform(self, df):
            df_borough = self.model_borough.transform(df)
            df_pc = self.model_pc.transform(df)
            borough_pred = df_borough.select("predictedBorough").collect()[0][
                0]  # Gi·∫£ s·ª≠ c·ªôt d·ª± ƒëo√°n l√† predictedBorough
            pc_pred = df_pc.select("predictedPassengerCount").collect()[0][0]
            return {"Borough": borough_pred, "passenger_count": int(pc_pred)}

    # H√†m d·ª± ƒëo√°n t·ª´ th·ªùi gian ƒë·∫ßu v√†o
    def predict_pickup(datetime_str, model):
        input_df = spark.createDataFrame([(datetime_str,)], ["tpep_pickup_datetime"])
        input_df = input_df.withColumn("tpep_pickup_datetime",
                                       to_timestamp(col("tpep_pickup_datetime"), "yyyy-MM-dd HH:mm:ss")) \
            .withColumn("hour", hour(col("tpep_pickup_datetime"))) \
            .withColumn("day_of_week", dayofweek(col("tpep_pickup_datetime"))) \
            .withColumn("day_of_month", dayofmonth(col("tpep_pickup_datetime"))) \
            .withColumn("month", month(col("tpep_pickup_datetime"))) \
            .withColumn("year", year(col("tpep_pickup_datetime")))
        return model.transform(input_df)

    # H√†m tra c·ª©u th√¥ng tin ƒë·ªãa ƒëi·ªÉm
    def get_location_info(location_id, lookup_df):
        location_info = lookup_df[lookup_df['LocationID'] == location_id]
        if not location_info.empty:
            return {
                "Borough": location_info.iloc[0]['Borough'],
                "Zone": location_info.iloc[0]['Zone'],
                "ServiceZone": location_info.iloc[0]['service_zone']
            }
        return None

    # CSS ƒë·ªÉ thu nh·ªè ti√™u ƒë·ªÅ trong sidebar v√† c√°c ph·∫ßn t·ª≠ kh√°c n·∫øu c·∫ßn
    st.markdown(
        """
        <style>
        /* Thu nh·ªè c√°c ti√™u ƒë·ªÅ trong sidebar */
        div[data-testid="stSidebar"] h1, div[data-testid="stSidebar"] h2, div[data-testid="stSidebar"] h3 {
            font-size: 12px !important;
            margin-top: 3px !important;
            margin-bottom: 3px !important;
        }
        /* Thu nh·ªè ch·ªØ c·ªßa c√°c ph·∫ßn t·ª≠ trong sidebar (n·∫øu c·∫ßn) */
        div[data-testid="stSidebar"] * {
            font-size: 12px !important;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    # Main application - S·ª≠ d·ª•ng st.markdown thay v√¨ st.title
    st.markdown(
        "<h1 style='font-size: 32px; margin-top: 3px; margin-bottom: 3px; padding-top:0px;'>Ph√¢n t√≠ch v√† D·ª± ƒëo√°n D·ªØ li·ªáu Taxi New York</h1>",
        unsafe_allow_html=True
    )

    # Sidebar navigation
    st.sidebar.markdown(
        "<h1 style='font-size: 30px; margin-top: 3px; margin-bottom: 3px;'>ƒêi·ªÅu h∆∞·ªõng</h1>",
        unsafe_allow_html=True
    )
    page = st.sidebar.radio("Ch·ªçn ch·ª©c nƒÉng", ["Ph√¢n t√≠ch d·ªØ li·ªáu th√¥", "Ph√¢n t√≠ch d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch", "D·ª± ƒëo√°n"])

    # Load common data
    zones = load_zone_lookup()
    geo_data = load_geojson()

    if page == "Ph√¢n t√≠ch d·ªØ li·ªáu th√¥":
        st.markdown(
            "<h1 style='font-size: 26px; margin-top: 4px; margin-bottom: 4px; padding-top:0px;'>Ph√¢n t√≠ch D·ªØ li·ªáu Taxi New York 2019-2020</h1>",
            unsafe_allow_html=True
        )

        # Sidebar inputs for year, month, time analysis, and analysis type
        st.sidebar.markdown(
            "<h1 style='font-size: 12px; margin-top: 3px; margin-bottom: 3px;'>Ch·ªçn b·ªô l·ªçc ph√¢n t√≠ch</h1>",
            unsafe_allow_html=True
        )

        # Year and Month selection
        year = st.sidebar.selectbox("NƒÉm", [2019, 2020], key="year_analysis")
        month_options = list(range(1, 13)) if year == 2019 else list(range(1, 7))
        month = st.sidebar.selectbox("Th√°ng", month_options, key="month_analysis")

        # Load data
        df = load_data(year, month)

        if df is not None and zones is not None and geo_data is not None:
            st.markdown(
                f"<h2 style='font-size: 20px; margin-top: 3px; margin-bottom: 3px; padding-top:0px;'>D·ªØ li·ªáu Taxi - Th√°ng {month}/{year}</h2>",
                unsafe_allow_html=True
            )

            # Time analysis option
            time_option = st.sidebar.selectbox(
                "Ch·ªçn ki·ªÉu ph√¢n t√≠ch th·ªùi gian",
                ["To√†n b·ªô th√°ng", "Theo tu·∫ßn", "Cu·ªëi tu·∫ßn"],
                key="time_option"
            )

            filtered_df = df

            if time_option == "Theo tu·∫ßn":
                weeks = get_weeks_in_month(df, year, month)
                if weeks:
                    week = st.sidebar.selectbox("Ch·ªçn tu·∫ßn", weeks, key="week")
                    start_date, end_date = get_week_date_range(year, week)
                    filtered_df = filter_by_week(df, year, month, week)
                    st.write(
                        f"Ph√¢n t√≠ch d·ªØ li·ªáu tu·∫ßn {week} (t·ª´ {start_date.strftime('%d/%m/%Y')} ƒë·∫øn {end_date.strftime('%d/%m/%Y')})")
                else:
                    st.error("Kh√¥ng c√≥ d·ªØ li·ªáu cho th√°ng n√†y!")
                    filtered_df = pd.DataFrame()

            elif time_option == "Cu·ªëi tu·∫ßn":
                weeks = get_weeks_in_month(df, year, month)
                if weeks:
                    week = st.sidebar.selectbox("Ch·ªçn tu·∫ßn", weeks, key="weekend")
                    saturday, sunday = get_weekend_dates(year, week)
                    filtered_df = filter_by_weekend(df, year, month, week)
                    st.write(
                        f"Ph√¢n t√≠ch d·ªØ li·ªáu cu·ªëi tu·∫ßn c·ªßa tu·∫ßn {week} (ng√†y {saturday.strftime('%d/%m/%Y')} v√† {sunday.strftime('%d/%m/%Y')})")
                else:
                    st.error("Kh√¥ng c√≥ d·ªØ li·ªáu cho th√°ng n√†y!")
                    filtered_df = pd.DataFrame()

            if not filtered_df.empty:
                # Analysis type selection
                analysis_option = st.sidebar.selectbox(
                    "Ch·ªçn lo·∫°i ph√¢n t√≠ch",
                    ["SHAPE", "Quan s√°t chi ti·∫øt c√°c c·ªôt d·ªØ li·ªáu", "Ki·ªÉm tra tr√πng l·∫∑p",
                     "Ki·ªÉm tra d·ªØ li·ªáu b·ªã nhi·ªÖu", "B·∫£n ƒë·ªì m·∫≠t ƒë·ªô ƒë√≥n kh√°ch"],
                    key="analysis_option"
                )

                # Display results based on analysis type
                if analysis_option == "SHAPE":
                    st.markdown(
                        "<h2 style='font-size: 20px; margin-top: 3px; margin-bottom: 3px;'>üìä K√≠ch th∆∞·ªõc d·ªØ li·ªáu</h2>",
                        unsafe_allow_html=True
                    )

                    # T·∫°o layout 2 c·ªôt
                    col1, col2 = st.columns(2)

                    with col1:
                        # S·ª≠ d·ª•ng st.metric cho s·ªë d√≤ng
                        st.metric(
                            label="**S·ªë d√≤ng**",
                            value=f"{filtered_df.shape[0]:,}",
                            help="T·ªïng s·ªë b·∫£n ghi trong t·∫≠p d·ªØ li·ªáu"
                        )

                    with col2:
                        # S·ª≠ d·ª•ng st.metric cho s·ªë c·ªôt
                        st.metric(
                            label="**S·ªë c·ªôt**",
                            value=filtered_df.shape[1],
                            help="T·ªïng s·ªë thu·ªôc t√≠nh trong t·∫≠p d·ªØ li·ªáu"
                        )



                elif analysis_option == "Quan s√°t chi ti·∫øt c√°c c·ªôt d·ªØ li·ªáu":

                    st.markdown(

                        "<h2 style='font-size: 12px; margin-top: 3px; margin-bottom: 3px;'>Chi ti·∫øt c√°c c·ªôt d·ªØ li·ªáu</h2>",

                        unsafe_allow_html=True

                    )

                    column = st.sidebar.selectbox("Ch·ªçn c·ªôt ƒë·ªÉ xem chi ti·∫øt", filtered_df.columns, key="column_detail")

                    # T·∫°o layout d·∫°ng card cho th√¥ng tin c∆° b·∫£n

                    col1, col2, col3 = st.columns(3)

                    with col1:

                        # Card hi·ªÉn th·ªã ki·ªÉu d·ªØ li·ªáu

                        dtype = str(filtered_df[column].dtype)

                        st.metric(

                            label="**Ki·ªÉu d·ªØ li·ªáu**",

                            value=dtype,

                            help=f"Ki·ªÉu d·ªØ li·ªáu c·ªßa c·ªôt {column}"

                        )

                    with col2:

                        # Card hi·ªÉn th·ªã s·ªë gi√° tr·ªã duy nh·∫•t

                        unique_count = filtered_df[column].nunique()

                        st.metric(

                            label="**Gi√° tr·ªã duy nh·∫•t**",

                            value=f"{unique_count:,}",

                            help=f"S·ªë l∆∞·ª£ng gi√° tr·ªã kh√¥ng tr√πng l·∫∑p trong c·ªôt {column}"

                        )

                    with col3:

                        # Card hi·ªÉn th·ªã s·ªë gi√° tr·ªã thi·∫øu

                        missing_count = filtered_df[column].isnull().sum()

                        missing_percent = (missing_count / len(filtered_df)) * 100

                        st.metric(

                            label="**Gi√° tr·ªã thi·∫øu**",

                            value=f"{missing_count:,} ({missing_percent:.1f}%)",

                            help=f"S·ªë l∆∞·ª£ng v√† t·ª∑ l·ªá gi√° tr·ªã thi·∫øu trong c·ªôt {column}"

                        )

                    # Ph·∫ßn bi·ªÉu ƒë·ªì

                    st.markdown("---")

                    st.markdown(

                        "<h3 style='font-size: 12px; margin-top: 3px; margin-bottom: 3px;'>Bi·ªÉu ƒë·ªì ph√¢n b·ªë</h3>",

                        unsafe_allow_html=True

                    )

                    # Danh s√°ch c√°c c·ªôt s·∫Ω v·∫Ω boxplot

                    boxplot_columns = [

                        'trip_distance',

                        'fare_amount',

                        'tip_amount',

                        'total_amount',

                        'tolls_amount'

                    ]

                    # T·∫°o figure

                    fig, ax = plt.subplots(figsize=(8, 4))

                    if column in ['tpep_pickup_datetime', 'tpep_dropoff_datetime']:

                        # X·ª≠ l√Ω c·ªôt th·ªùi gian - v·∫Ω histogram theo gi·ªù

                        filtered_df['hour'] = filtered_df[column].dt.hour

                        sns.histplot(data=filtered_df, x='hour', bins=24, kde=True, ax=ax)

                        ax.set_xlabel(f"Gi·ªù trong ng√†y ({column})")

                        ax.set_ylabel("S·ªë l∆∞·ª£ng chuy·∫øn ƒëi")

                        plt.xticks(range(0, 24))


                    elif column in boxplot_columns:

                        # X·ª≠ l√Ω c√°c c·ªôt s·ªë - v·∫Ω boxplot

                        sns.boxplot(x=filtered_df[column], ax=ax)

                        ax.set_xlabel(column)

                        ax.set_title(f"Ph√¢n ph·ªëi gi√° tr·ªã {column}")


                    else:

                        # X·ª≠ l√Ω c√°c c·ªôt c√≤n l·∫°i - v·∫Ω countplot

                        if filtered_df[column].nunique() <= 20:

                            sns.countplot(x=filtered_df[column], ax=ax)

                            ax.set_xlabel(column)

                            ax.set_ylabel("S·ªë l∆∞·ª£ng")

                            plt.xticks(rotation=45)

                        else:

                            st.warning(
                                "C·ªôt n√†y c√≥ qu√° nhi·ªÅu gi√° tr·ªã duy nh·∫•t ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì. Vui l√≤ng ch·ªçn c·ªôt kh√°c.")

                    plt.tight_layout()

                    st.pyplot(fig, clear_figure=True)

                    plt.close(fig)


                elif analysis_option == "Ki·ªÉm tra tr√πng l·∫∑p":

                    st.markdown(

                        """

                        <h2 style='font-size: 16px; margin-bottom: 10px;'>

                        üîç Ki·ªÉm tra d·ªØ li·ªáu tr√πng l·∫∑p

                        </h2>

                        """,

                        unsafe_allow_html=True

                    )

                    # T·∫°o card hi·ªÉn th·ªã k·∫øt qu·∫£

                    duplicate_card = st.container()

                    duplicated_rows = filtered_df[filtered_df.duplicated(keep=False)]

                    num_duplicates = len(duplicated_rows)

                    if num_duplicates > 0:

                        duplicate_card.warning(

                            f"‚ö†Ô∏è Ph√°t hi·ªán **{num_duplicates}** b·∫£n ghi tr√πng l·∫∑p",

                            icon="‚ö†Ô∏è"

                        )

                        # Hi·ªÉn th·ªã th·ªëng k√™

                        with st.expander("üìä Th·ªëng k√™ chi ti·∫øt", expanded=True):

                            cols = st.columns(2)

                            cols[0].metric("T·ªïng b·∫£n ghi", len(filtered_df))

                            cols[1].metric("B·∫£n ghi tr√πng", num_duplicates)

                        # Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu tr√πng

                        with st.expander("üîé Xem d·ªØ li·ªáu tr√πng l·∫∑p", expanded=False):

                            st.dataframe(

                                duplicated_rows.sort_values(by=filtered_df.columns.tolist()).head(20),

                                height=300,

                                use_container_width=True

                            )

                        # N√∫t t·∫£i v·ªÅ d·ªØ li·ªáu tr√πng

                        csv = duplicated_rows.to_csv(index=False).encode('utf-8')

                        st.download_button(

                            label="üì• T·∫£i v·ªÅ d·ªØ li·ªáu tr√πng",

                            data=csv,

                            file_name=f"duplicated_records_{year}_{month}.csv",

                            mime='text/csv'

                        )

                    else:

                        duplicate_card.success(

                            "‚úÖ Kh√¥ng c√≥ b·∫£n ghi n√†o tr√πng l·∫∑p!",

                            icon="‚úÖ"

                        )

                        # Hi·ªÉn th·ªã th·ªëng k√™ t√≠ch c·ª±c

                        with st.expander("üìä Th·ªëng k√™ d·ªØ li·ªáu s·∫°ch", expanded=True):

                            cols = st.columns(2)

                            cols[0].metric("T·ªïng b·∫£n ghi", len(filtered_df))

                            cols[1].metric("B·∫£n ghi tr√πng", 0)


                elif analysis_option == "Ki·ªÉm tra d·ªØ li·ªáu b·ªã nhi·ªÖu":
                    def show_noise_details(noise_data, title, description):
                        """Hi·ªÉn th·ªã chi ti·∫øt t·ª´ng lo·∫°i nhi·ªÖu"""
                        if noise_data['count'] > 0:
                            st.markdown(f"""
                            ### {title}
                            *{description}*  
                            **S·ªë l∆∞·ª£ng:** {noise_data['count']:,} b·∫£n ghi  
                            """)

                            # Hi·ªÉn th·ªã 10 b·∫£n ghi ƒë·∫ßu ti√™n
                            with st.expander("üìã Xem chi ti·∫øt b·∫£n ghi", expanded=False):
                                cols = st.columns(2)
                                cols[0].write("**Index c√°c b·∫£n ghi:**")
                                cols[0].write(noise_data['indices'][:10])

                                # Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u
                                sample_data = filtered_df.loc[noise_data['indices'][:5]]
                                cols[1].write("**D·ªØ li·ªáu m·∫´u:**")
                                cols[1].dataframe(sample_data, height=200)

                                # N√∫t t·∫£i v·ªÅ
                                csv = sample_data.to_csv(index=False).encode('utf-8')
                                st.download_button(
                                    label=f"üì• T·∫£i v·ªÅ m·∫´u d·ªØ li·ªáu {title.lower()}",
                                    data=csv,
                                    file_name=f"noisy_data_{title}.csv",
                                    mime='text/csv'
                                )
                        else:
                            st.success(f"‚úÖ Kh√¥ng c√≥ b·∫£n ghi {title.lower()}", icon="‚úÖ")

                    st.markdown("""

                    <h2 style='font-size: 16px; margin-bottom: 10px;'>

                    üßπ Ki·ªÉm tra d·ªØ li·ªáu nhi·ªÖu

                    </h2>

                    """, unsafe_allow_html=True)

                    noise_info = check_noisy_data(filtered_df)

                    total_noise = sum(info['count'] for info in noise_info.values())

                    # T·∫°o card t·ªïng quan

                    if total_noise > 0:

                        st.warning(f"""

                        ‚ö†Ô∏è **Ph√°t hi·ªán {total_noise:,} b·∫£n ghi c√≥ d·ªØ li·ªáu nhi·ªÖu**  

                        ({(total_noise / len(filtered_df) * 100):.2f}% t·ªïng s·ªë b·∫£n ghi)

                        """, icon="‚ö†Ô∏è")

                    else:

                        st.success("‚úÖ Kh√¥ng ph√°t hi·ªán d·ªØ li·ªáu nhi·ªÖu", icon="‚úÖ")

                    # T·∫°o tabs cho t·ª´ng lo·∫°i nhi·ªÖu

                    tab1, tab2, tab3, tab4 = st.tabs([

                        "‚è±Ô∏è Th·ªùi gian",

                        "üõ£Ô∏è Qu√£ng ƒë∆∞·ªùng",

                        "üë• H√†nh kh√°ch",

                        "üí∞ Gi√° ti·ªÅn"

                    ])

                    with tab1:

                        show_noise_details(

                            noise_info['invalid_time'],

                            "Th·ªùi gian b·∫•t th∆∞·ªùng",

                            "Chuy·∫øn ƒëi c√≥ th·ªùi gian √¢m ho·∫∑c k√©o d√†i > 1 ng√†y"

                        )

                    with tab2:

                        show_noise_details(

                            noise_info['invalid_distance'],

                            "Qu√£ng ƒë∆∞·ªùng b·∫•t th∆∞·ªùng",

                            "Chuy·∫øn ƒëi c√≥ qu√£ng ƒë∆∞·ªùng < 0 ho·∫∑c > 100 d·∫∑m"

                        )

                    with tab3:

                        show_noise_details(

                            noise_info['invalid_passengers'],

                            "S·ªë h√†nh kh√°ch b·∫•t th∆∞·ªùng",

                            "Chuy·∫øn ƒëi c√≥ s·ªë kh√°ch ‚â§ 0 ho·∫∑c > 6 ng∆∞·ªùi"

                        )

                    with tab4:

                        show_noise_details(

                            noise_info['invalid_fare'],

                            "Gi√° ti·ªÅn b·∫•t th∆∞·ªùng",

                            "Chuy·∫øn ƒëi c√≥ gi√° ti·ªÅn < 0 ho·∫∑c > 1000 USD"

                        )


                elif analysis_option == "B·∫£n ƒë·ªì m·∫≠t ƒë·ªô ƒë√≥n kh√°ch":

                    st.markdown("""

                    <h2 style='font-size: 16px; margin-bottom: 10px;'>

                    üó∫Ô∏è B·∫£n ƒë·ªì m·∫≠t ƒë·ªô ƒë√≥n kh√°ch

                    </h2>

                    """, unsafe_allow_html=True)

                    # T·∫°o layout 2 c·ªôt

                    col1, col2 = st.columns([3, 1])

                    with col1:

                        # T√≠nh to√°n d·ªØ li·ªáu ƒë√≥n kh√°ch

                        pickup_counts = filtered_df['PULocationID'].value_counts().reset_index()

                        pickup_counts.columns = ['LocationID', 'pickup_count']

                        # K·∫øt h·ª£p v·ªõi d·ªØ li·ªáu ƒë·ªãa l√Ω

                        geo_data['location_id'] = geo_data['location_id'].astype(int)

                        map_data = geo_data.merge(pickup_counts, left_on='location_id', right_on='LocationID',
                                                  how='left')

                        map_data['pickup_count'] = map_data['pickup_count'].fillna(0)

                        # T·∫°o b·∫£n ƒë·ªì Folium

                        m = folium.Map(location=[40.7128, -74.0060],

                                       zoom_start=11,

                                       tiles="cartodbpositron",

                                       width='100%',

                                       height=500)

                        # Th√™m l·ªõp choropleth

                        choropleth = folium.Choropleth(

                            geo_data=map_data,

                            name='choropleth',

                            data=map_data,

                            columns=['LocationID', 'pickup_count'],

                            key_on='feature.properties.location_id',

                            fill_color='YlOrRd',

                            fill_opacity=0.7,

                            line_opacity=0.2,

                            legend_name='S·ªë l∆∞·ª£ng chuy·∫øn ƒëi',

                            highlight=True

                        ).add_to(m)

                        # Th√™m tooltip

                        folium.GeoJson(

                            map_data,

                            style_function=lambda x: {'fillColor': '#ffffff00', 'color': '#000000', 'weight': 1},

                            tooltip=folium.GeoJsonTooltip(

                                fields=['zone', 'borough', 'pickup_count'],

                                aliases=['Khu v·ª±c:', 'Qu·∫≠n:', 'S·ªë chuy·∫øn ƒëi:'],

                                localize=True,

                                style=("font-weight: bold;")

                            )

                        ).add_to(m)

                        # Th√™m layer control

                        folium.LayerControl().add_to(m)

                        # Hi·ªÉn th·ªã b·∫£n ƒë·ªì

                        st_folium(m, use_container_width=True)

                    with col2:

                        # Panel th·ªëng k√™

                        st.markdown("### üìä Th·ªëng k√™")

                        total_pickups = int(map_data['pickup_count'].sum())

                        avg_pickups = int(map_data['pickup_count'].mean())

                        max_pickups = int(map_data['pickup_count'].max())

                        max_zone = map_data.loc[map_data['pickup_count'].idxmax(), 'zone']

                        st.metric("T·ªïng chuy·∫øn ƒëi", f"{total_pickups:,}")

                        st.metric("Trung b√¨nh/khu v·ª±c", f"{avg_pickups:,}")

                        st.metric("Khu v·ª±c ƒë√¥ng nh·∫•t",

                                  f"{max_pickups:,}",

                                  f"{max_zone}")

                        # Top 5 khu v·ª±c

                        st.markdown("**Top 5 khu v·ª±c:**")

                        top_zones = map_data.sort_values('pickup_count', ascending=False).head(5)

                        for _, row in top_zones.iterrows():
                            st.write(f"- {row['zone']}: {int(row['pickup_count']):,}")
            else:
                st.error("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch cho l·ª±a ch·ªçn n√†y!")
        else:
            st.error("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu. Vui l√≤ng ki·ªÉm tra c√°c file ƒë·∫ßu v√†o.")

    elif page == "D·ª± ƒëo√°n":
        st.markdown("""
            <h1 style='font-size: 18px; margin-bottom: 20px;'>
            üöñ D·ª± ƒëo√°n s·ªë l∆∞·ª£ng chuy·∫øn xe Taxi
            </h1>
            """, unsafe_allow_html=True)

        col1, col2 = st.columns([1, 2])

        with col1:
            with st.container():
                st.markdown("### ‚öôÔ∏è Thi·∫øt l·∫≠p d·ª± ƒëo√°n")

                with st.expander("üìÖ Ch·ªçn th√¥ng tin", expanded=True):
                    boroughs = ['Manhattan', 'Brooklyn', 'Queens', 'Bronx', 'EWR']
                    selected_date = st.date_input(
                        "Ng√†y d·ª± ƒëo√°n",
                        value=datetime(2020, 4, 1),
                        min_value=datetime(2020, 1, 1),
                        max_value=datetime(2020, 12, 31)
                    )
                    selected_borough = st.selectbox("Qu·∫≠n", options=boroughs, index=0)
                    selected_hour = st.selectbox("Gi·ªù", options=range(24), format_func=lambda x: f"{x:02d}", index=8)

                predict_btn = st.button("üîÆ Ch·∫°y d·ª± ƒëo√°n", use_container_width=True, type="primary")

        with col2:
            result_container = st.container()
            result_container.markdown("### üìä K·∫øt qu·∫£ d·ª± ƒëo√°n")

            if predict_btn:
                try:
                    with st.spinner("ƒêang x·ª≠ l√Ω d·ª± ƒëo√°n..."):
                        # T·∫£i m√¥ h√¨nh
                        model = load_model_from_hdfs(selected_borough)
                        # T·∫£i d·ªØ li·ªáu l·ªãch s·ª≠ v√† b·ªô chu·∫©n h√≥a
                        pdf = load_historical_data(selected_borough)
                        # D·ª± ƒëo√°n
                        predicted_trips, predictions, date_hour_str = predict_trips(model, pdf, selected_date,
                                                                                    selected_hour)
                        # L·∫•y ƒë·ªô ƒëo hi·ªáu su·∫•t t·ª´ HARDCODED_METRICS
                        metrics = HARDCODED_METRICS.get(selected_borough,
                                                        {'mae': 0.0, 'mse': 0.0, 'rmse': 0.0, 'r2': 0.0})

                    result_container.success("‚úÖ D·ª± ƒëo√°n th√†nh c√¥ng!")

                    st.metric(
                        label="**S·ªë chuy·∫øn xe d·ª± ƒëo√°n**",
                        value=predicted_trips,
                        help=f"S·ªë l∆∞·ª£ng chuy·∫øn xe d·ª± ƒëo√°n t·∫°i {selected_borough} v√†o {selected_date} {selected_hour:02d}:00"
                    )

                    # V·∫Ω bi·ªÉu ƒë·ªì ƒë∆∞·ªùng
                    fig = px.line(
                        x=date_hour_str,
                        y=predictions,
                        labels={'x': 'Ng√†y v√† Gi·ªù', 'y': 'S·ªë chuy·∫øn xe d·ª± ƒëo√°n'},
                        title=f"D·ª± ƒëo√°n s·ªë chuy·∫øn xe t·∫°i {selected_borough} t·ª´ {selected_hour:02d}:00",
                        markers=True
                    )
                    fig.update_traces(line=dict(color='#1f77b4', width=2), marker=dict(size=8))
                    fig.update_layout(
                        xaxis_title="Ng√†y v√† Gi·ªù",
                        yaxis_title="S·ªë chuy·∫øn xe",
                        font=dict(size=12),
                        title_font_size=16,
                        showlegend=False,
                        plot_bgcolor='rgba(0,0,0,0)',
                        paper_bgcolor='rgba(0,0,0,0)',
                        xaxis=dict(showgrid=True, gridcolor='rgba(200,200,200,0.3)'),
                        yaxis=dict(showgrid=True, gridcolor='rgba(200,200,200,0.3)')
                    )
                    st.plotly_chart(fig, use_container_width=True)

                    with st.expander("üìà ƒê·ªô ch√≠nh x√°c m√¥ h√¨nh", expanded=True):
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("MAE", f"{metrics['mae']:.4f}")
                        with col2:
                            st.metric("MSE", f"{metrics['mse']:.4f}")
                        with col3:
                            st.metric("RMSE", f"{metrics['rmse']:.4f}")
                        with col4:
                            st.metric("R¬≤", f"{metrics['r2']:.4f}")

                except Exception as e:
                    result_container.error(f"‚ùå L·ªói khi d·ª± ƒëo√°n: {str(e)}")

    elif page == "Ph√¢n t√≠ch d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch":
        st.markdown("""
        <h1 style='font-size: 18px; margin-bottom: 20px;'>
        üßπ Ph√¢n t√≠ch d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch 
        </h1>
        """, unsafe_allow_html=True)

        # Sidebar inputs for date range
        st.sidebar.markdown(
            "<h1 style='font-size: 12px; margin-top: 3px; margin-bottom: 3px;'>Ch·ªçn kho·∫£ng th·ªùi gian</h1>",
            unsafe_allow_html=True
        )

        # Date range selection in sidebar
        start_date = st.sidebar.date_input(
            "Ng√†y b·∫Øt ƒë·∫ßu",
            value=datetime(2020, 1, 1),
            min_value=datetime(2019, 1, 1),
            max_value=datetime(2020, 12, 31),
            key="clean_start_date"
        )

        end_date = st.sidebar.date_input(
            "Ng√†y k·∫øt th√∫c",
            value=datetime(2020, 1, 31),
            min_value=datetime(2019, 1, 1),
            max_value=datetime(2020, 12, 31),
            key="clean_end_date"
        )

        # Validate date range
        if start_date > end_date:
            st.error("Ng√†y b·∫Øt ƒë·∫ßu ph·∫£i nh·ªè h∆°n ho·∫∑c b·∫±ng ng√†y k·∫øt th√∫c")
            st.stop()

        # Load data
        with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu t·ª´ Iceberg..."):
            df = query_iceberg_data(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))

        if df is not None:
            if not df.empty:
                st.success(
                    f"ƒê√£ t·∫£i {len(df)} b·∫£n ghi t·ª´ {start_date.strftime('%d/%m/%Y')} ƒë·∫øn {end_date.strftime('%d/%m/%Y')}")

                # Hi·ªÉn th·ªã c√°c ch·ªâ s·ªë t·ªïng quan
                st.markdown("### üìä Ch·ªâ s·ªë t·ªïng quan")

                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    st.metric("T·ªïng s·ªë chuy·∫øn ƒëi", len(df))

                with col2:
                    avg_passengers = df['passenger_count'].mean()
                    st.metric("S·ªë kh√°ch trung b√¨nh", f"{avg_passengers:.1f}")

                with col3:
                    avg_distance = df['trip_distance'].mean()
                    st.metric("Qu√£ng ƒë∆∞·ªùng TB (d·∫∑m)", f"{avg_distance:.1f}")

                with col4:
                    avg_fare = df['total_amount'].mean()
                    st.metric("Gi√° trung b√¨nh (USD)", f"${avg_fare:.2f}")

                # Ph√¢n t√≠ch theo c√°c kh√≠a c·∫°nh kh√°c nhau
                st.markdown("---")
                st.markdown("### üìà Ph√¢n t√≠ch chi ti·∫øt")

                # T·∫°o tab v·ªõi radio buttons ƒë·ªÉ duy tr√¨ tr·∫°ng th√°i
                tab_options = ["üïí Theo gi·ªù", "üó∫Ô∏è Theo khu v·ª±c", "üí∞ Gi√° v√©", "üìÖ Theo ng√†y"]
                current_tab = st.radio("Ch·ªçn ch·∫ø ƒë·ªô ph√¢n t√≠ch:", tab_options, horizontal=True)

                # Hi·ªÉn th·ªã n·ªôi dung t∆∞∆°ng ·ª©ng v·ªõi tab ƒë∆∞·ª£c ch·ªçn
                if current_tab == "üïí Theo gi·ªù":
                    # Ph√¢n b·ªë theo gi·ªù
                    df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour
                    hourly_counts = df['pickup_hour'].value_counts().sort_index()

                    fig, ax = plt.subplots(figsize=(10, 4))
                    sns.barplot(x=hourly_counts.index, y=hourly_counts.values, ax=ax)
                    ax.set_title("S·ªë l∆∞·ª£ng chuy·∫øn ƒëi theo gi·ªù trong ng√†y")
                    ax.set_xlabel("Gi·ªù")
                    ax.set_ylabel("S·ªë chuy·∫øn ƒëi")
                    st.pyplot(fig)
                    plt.close(fig)

                    # Top 5 gi·ªù cao ƒëi·ªÉm
                    st.write("**Top 5 gi·ªù cao ƒëi·ªÉm:**")
                    top_hours = hourly_counts.nlargest(5)
                    for hour, count in top_hours.items():
                        st.write(f"- {hour}h: {count} chuy·∫øn")

                elif current_tab == "üó∫Ô∏è Theo khu v·ª±c":
                    if zones is not None and geo_data is not None:
                        # Ph√¢n b·ªë theo khu v·ª±c ƒë√≥n
                        pu_counts = df['PULocationID'].value_counts().reset_index()
                        pu_counts.columns = ['LocationID', 'pickup_count']

                        # K·∫øt h·ª£p v·ªõi th√¥ng tin zone
                        pu_counts = pu_counts.merge(
                            zones,
                            left_on='LocationID',
                            right_on='LocationID',
                            how='left'
                        )

                        # Hi·ªÉn th·ªã top 10 khu v·ª±c
                        st.write("**Top 10 khu v·ª±c ƒë√≥n kh√°ch nhi·ªÅu nh·∫•t:**")
                        top_pu = pu_counts.nlargest(10, 'pickup_count')
                        st.dataframe(top_pu[['Zone', 'Borough', 'pickup_count']].rename(
                            columns={'Zone': 'Khu v·ª±c', 'Borough': 'Qu·∫≠n', 'pickup_count': 'S·ªë chuy·∫øn'}
                        ))

                        # B·∫£n ƒë·ªì nhi·ªát
                        geo_data['location_id'] = geo_data['location_id'].astype(int)
                        map_data = geo_data.merge(
                            pu_counts,
                            left_on='location_id',
                            right_on='LocationID',
                            how='left'
                        )
                        map_data['pickup_count'] = map_data['pickup_count'].fillna(0)

                        # T·∫°o b·∫£n ƒë·ªì Folium v·ªõi k√≠ch th∆∞·ªõc nh·ªè h∆°n
                        m = folium.Map(location=[40.7128, -74.0060],
                                       zoom_start=11,
                                       tiles="cartodbpositron")

                        # Th√™m l·ªõp choropleth
                        choropleth = folium.Choropleth(
                            geo_data=map_data,
                            name='choropleth',
                            data=map_data,
                            columns=['LocationID', 'pickup_count'],
                            key_on='feature.properties.location_id',
                            fill_color='YlOrRd',
                            fill_opacity=0.7,
                            line_opacity=0.2,
                            legend_name='S·ªë l∆∞·ª£ng chuy·∫øn ƒëi',
                            highlight=True
                        ).add_to(m)

                        # Th√™m tooltip
                        folium.GeoJson(
                            map_data,
                            style_function=lambda x: {'fillColor': '#ffffff00', 'color': '#000000', 'weight': 1},
                            tooltip=folium.GeoJsonTooltip(
                                fields=['zone', 'borough', 'pickup_count'],
                                aliases=['Khu v·ª±c:', 'Qu·∫≠n:', 'S·ªë chuy·∫øn ƒëi:'],
                                localize=True,
                                style=("font-weight: bold;")
                            )
                        ).add_to(m)

                        # Th√™m layer control
                        folium.LayerControl().add_to(m)

                        # Hi·ªÉn th·ªã b·∫£n ƒë·ªì v·ªõi k√≠ch th∆∞·ªõc nh·ªè h∆°n v√† √≠t t√πy ch·ªçn t∆∞∆°ng t√°c h∆°n
                        st_folium(m, width=700, height=450)

                        # Th·ªëng k√™ khu v·ª±c
                        st.markdown("### üìä Th·ªëng k√™ khu v·ª±c")
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("T·ªïng s·ªë khu v·ª±c c√≥ ƒë√≥n kh√°ch",
                                      len(pu_counts[pu_counts['pickup_count'] > 0]))
                        with col2:
                            st.metric("Khu v·ª±c c√≥ nhi·ªÅu chuy·∫øn nh·∫•t",
                                      f"{top_pu.iloc[0]['Zone']} ({top_pu.iloc[0]['pickup_count']} chuy·∫øn)")

                elif current_tab == "üí∞ Gi√° v√©":
                    # Ph√¢n t√≠ch gi√° v√©
                    st.markdown("### üìä Ph√¢n b·ªë gi√° v√©")

                    # T·∫°o 2 c·ªôt cho bi·ªÉu ƒë·ªì
                    col1, col2 = st.columns(2)

                    with col1:
                        # Boxplot gi√° v√©
                        fig1, ax1 = plt.subplots(figsize=(8, 4))
                        sns.boxplot(x=df['total_amount'], ax=ax1)
                        ax1.set_title("Ph√¢n ph·ªëi gi√° v√©")
                        ax1.set_xlabel("Gi√° v√© (USD)")
                        st.pyplot(fig1)
                        plt.close(fig1)

                    with col2:
                        # Histogram gi√° v√©
                        fig2, ax2 = plt.subplots(figsize=(8, 4))
                        sns.histplot(df['total_amount'], bins=30, kde=True, ax=ax2)
                        ax2.set_title("Histogram gi√° v√©")
                        ax2.set_xlabel("Gi√° v√© (USD)")
                        st.pyplot(fig2)
                        plt.close(fig2)

                    # Th·ªëng k√™ chi ti·∫øt
                    st.markdown("### üìà Th·ªëng k√™ gi√° v√©")
                    fare_stats = df['total_amount'].describe().to_frame().T
                    st.dataframe(fare_stats.style.format("{:.2f}"))

                    # Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa qu√£ng ƒë∆∞·ªùng v√† gi√° v√©
                    st.markdown("### üîó M·ªëi quan h·ªá qu√£ng ƒë∆∞·ªùng - gi√° v√©")
                    fig3, ax3 = plt.subplots(figsize=(10, 5))
                    sns.scatterplot(data=df.sample(min(1000, len(df))),
                                    x='trip_distance', y='total_amount', alpha=0.5, ax=ax3)
                    ax3.set_title("Quan h·ªá gi·ªØa qu√£ng ƒë∆∞·ªùng v√† gi√° v√©")
                    ax3.set_xlabel("Qu√£ng ƒë∆∞·ªùng (d·∫∑m)")
                    ax3.set_ylabel("Gi√° v√© (USD)")
                    st.pyplot(fig3)
                    plt.close(fig3)

                elif current_tab == "üìÖ Theo ng√†y":
                    # Ph√¢n t√≠ch theo ng√†y
                    df['pickup_date'] = df['tpep_pickup_datetime'].dt.date
                    daily_counts = df['pickup_date'].value_counts().sort_index()

                    # Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng
                    st.markdown("### üìÖ Xu h∆∞·ªõng theo ng√†y")
                    fig1, ax1 = plt.subplots(figsize=(12, 4))
                    sns.lineplot(x=daily_counts.index, y=daily_counts.values, ax=ax1)
                    ax1.set_title("S·ªë l∆∞·ª£ng chuy·∫øn ƒëi theo ng√†y")
                    ax1.set_xlabel("Ng√†y")
                    ax1.set_ylabel("S·ªë chuy·∫øn ƒëi")
                    plt.xticks(rotation=45)
                    st.pyplot(fig1)
                    plt.close(fig1)

                    # Ph√¢n t√≠ch theo ng√†y trong tu·∫ßn
                    st.markdown("### üìÜ Ph√¢n b·ªë theo ng√†y trong tu·∫ßn")
                    df['day_of_week'] = df['tpep_pickup_datetime'].dt.day_name()
                    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
                    day_counts = df['day_of_week'].value_counts().reindex(day_order)

                    fig2, ax2 = plt.subplots(figsize=(10, 4))
                    sns.barplot(x=day_counts.index, y=day_counts.values, ax=ax2)
                    ax2.set_title("S·ªë chuy·∫øn ƒëi theo ng√†y trong tu·∫ßn")
                    ax2.set_xlabel("Ng√†y")
                    ax2.set_ylabel("S·ªë chuy·∫øn ƒëi")
                    plt.xticks(rotation=45)
                    st.pyplot(fig2)
                    plt.close(fig2)

                    # Top 5 ng√†y cao ƒëi·ªÉm
                    st.markdown("### üèÜ Top 5 ng√†y cao ƒëi·ªÉm")
                    top_days = daily_counts.nlargest(5)
                    for day, count in top_days.items():
                        st.write(f"- **{day.strftime('%d/%m/%Y')}**: {count} chuy·∫øn")

                # N√∫t t·∫£i d·ªØ li·ªáu
                st.markdown("---")
                st.download_button(
                    label="üì• T·∫£i d·ªØ li·ªáu ph√¢n t√≠ch",
                    data=df.to_csv(index=False).encode('utf-8'),
                    file_name=f"cleaned_taxi_data_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}.csv",
                    mime='text/csv'
                )
            else:
                st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng th·ªùi gian ƒë√£ ch·ªçn")

    class CombinedModel:
        def __init__(self, model_pu, model_pc):
            self.model_pu = model_pu
            self.model_pc = model_pc

        def transform(self, df):
            df_pu = self.model_pu.transform(df)
            df_pc = self.model_pc.transform(df)
            pu_pred = df_pu.select("predictedBorough").collect()[0][0]
            pc_pred = df_pc.select("predictedPassengerCount").collect()[0][0]
            return {"PULocationID": int(pu_pred), "passenger_count": int(pc_pred)}

    # Cleanup SparkSession
    def cleanup():
        spark.stop()

    # Register cleanup
    if hasattr(st, 'experimental_memo'):
        st.experimental_memo.clear()
    if hasattr(st, 'cache_data'):
        st.cache_data.clear()
    if hasattr(st, 'cache_resource'):
        st.cache_resource.clear()


# H√†m ch√≠nh c·ªßa ·ª©ng d·ª•ng
def main():
    # Ki·ªÉm tra tr·∫°ng th√°i ƒëƒÉng nh·∫≠p
    if "logged_in" not in st.session_state:
        st.session_state.logged_in = False

    if not st.session_state.logged_in:
        st.markdown("<h1 style='font-size: 32px; margin-top: 3px; margin-bottom: 3px; padding-top:0px;'>ƒêƒÉng nh·∫≠p</h1>",
                    unsafe_allow_html=True)
        username = st.text_input("T√™n ng∆∞·ªùi d√πng")
        password = st.text_input("M·∫≠t kh·∫©u", type="password")

        if st.button("ƒêƒÉng nh·∫≠p"):
            if check_login(username, password):
                st.session_state.logged_in = True
                st.success("ƒêƒÉng nh·∫≠p th√†nh c√¥ng!")
                st.rerun()  # T·∫£i l·∫°i trang ƒë·ªÉ hi·ªÉn th·ªã giao di·ªán ch√≠nh
            else:
                st.error("T√™n ng∆∞·ªùi d√πng ho·∫∑c m·∫≠t kh·∫©u kh√¥ng ƒë√∫ng!")
    else:
        main_page()


if __name__ == "__main__":
    main()


